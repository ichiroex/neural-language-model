loading training data...
txt/bb.train.txt
dataset size 28000
symbol vocab size: 3552
symbol vocab size(actual): 3549

epoch: 0 / 10
train mean loss=139.387391434
training perplexity=32.6121456795
saving model....
epoch: 1 / 10
train mean loss=108.042944074
training perplexity=14.8957152109
saving model....
epoch: 2 / 10
train mean loss=91.2585397993
training perplexity=9.79099900729
saving model....
epoch: 3 / 10
train mean loss=82.1475018365
training perplexity=7.7965984772
saving model....
epoch: 4 / 10
train mean loss=77.2368842316
training perplexity=6.89586602876
saving model....
epoch: 5 / 10
train mean loss=74.1715485655
training perplexity=6.3871535425
saving model....
epoch: 6 / 10
train mean loss=71.9733055823
training perplexity=6.04561151587
saving model....
epoch: 7 / 10
train mean loss=70.2605629567
training perplexity=5.79221094172
saving model....
epoch: 8 / 10
train mean loss=68.8703973716
training perplexity=5.59436552116
saving model....
epoch: 9 / 10
train mean loss=67.6940113068
training perplexity=5.43223299658
saving model....
